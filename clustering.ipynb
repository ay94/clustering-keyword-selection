{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Inter-Class Clustering Notebook\n",
    "\n",
    "## Setting up and preparing the data for clustering"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "#Import all necessary packages\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from sentence_transformers import SentenceTransformer, models\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.decomposition import PCA\n",
    "import umap.umap_ as umap\n",
    "import seaborn as sns\n",
    "import hdbscan\n",
    "from math import *\n",
    "from statistics import *\n",
    "pd.set_option('mode.chained_assignment', None)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Loading in the abstracts dataset\n",
    "abstracts = pd.read_csv(\"datasets/science.csv\")\n",
    "\n",
    "#Check data encoding\n",
    "print(abstracts.head())\n",
    "#Need to change the encodings\n",
    "#Concat the encoding columns into string\n",
    "abstracts['labels'] = abstracts[abstracts.columns[3:]].apply(lambda x: \"\".join(x.astype(str)), axis = 1)\n",
    "\n",
    "#Check class balance\n",
    "print(abstracts.labels.value_counts())\n",
    "\n",
    "#Seems like there are cases where entries belong to multiple classes, remove those and just keep single members\n",
    "labels_mapping = {\"010000\" : 0, \"100000\" : 1, \"001000\" : 2, \"000100\" : 3, \"000010\" : 4, \"000001\" : 5}\n",
    "\n",
    "abstracts = abstracts[abstracts.labels.isin(list(labels_mapping.keys()))]\n",
    "abstracts.reset_index(drop=True, inplace= True)\n",
    "abstracts.labels = abstracts.labels.apply(lambda x: labels_mapping[x])\n",
    "\n",
    "#Rename\n",
    "abstracts.rename(columns={\"ABSTRACT\": \"text1\"}, inplace= True)\n",
    "abstracts.rename(columns={\"TITLE\": \"text\"}, inplace= True)\n",
    "\n",
    "#Only preprocessing can be removing the new-line character (\\n)\n",
    "abstracts.text = abstracts.text.apply(lambda x: x.replace(\"\\n\", \" \"))\n",
    "abstracts.text1 = abstracts.text1.apply(lambda x: x.replace(\"\\n\", \" \"))\n",
    "\n",
    "#Lets check again the longest abstract entries:\n",
    "lengths_title = abstracts.text.str.len()\n",
    "argmax = np.where(lengths_title == lengths_title.max())[0]\n",
    "\n",
    "#Check length of longest title\n",
    "print(abstracts.text.iloc[argmax].to_numpy().ravel().tolist())\n",
    "print(len(abstracts.text.iloc[argmax].to_numpy().ravel().tolist()[0].split()))\n",
    "\n",
    "lengths_abstract = abstracts.text1.str.len()\n",
    "argmax = np.where(lengths_abstract == lengths_abstract.max())[0]\n",
    "\n",
    "#Check length of longest title\n",
    "print(abstracts.text1.iloc[argmax].to_numpy().ravel().tolist())\n",
    "print(len(abstracts.text1.iloc[argmax].to_numpy().ravel().tolist()[0].split()))\n",
    "\n",
    "\n",
    "#Finally lets keep only the columns we are interested in further\n",
    "abstracts = abstracts[['text', 'text1', 'labels']]\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Getting the embeddings"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "#Helper functions for saving/loading pickle objects\n",
    "def save_obj(obj, name ):\n",
    "    with open('datasets/'+ name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name ):\n",
    "    with open('datasets/' + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "#getting sentence embeddings for all the separated questions\n",
    "def get_sentence_embeddings(dataset):\n",
    "    \n",
    "    word_embedding_model = models.Transformer('sentence-transformers/all-mpnet-base-v2', max_seq_length=384)\n",
    "    pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n",
    "\n",
    "    model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n",
    "\n",
    "    #Store the embeddings in a list\n",
    "    sentence_embeddings = []\n",
    "\n",
    "    #Go through the data and get the sentence embeddings\n",
    "    for index in tqdm(range(len(dataset))):\n",
    "        sentence = dataset.text[index]\n",
    "        embedding = model.encode(sentence)\n",
    "        sentence_embeddings.append(embedding)\n",
    "    \n",
    "\n",
    "    dataset['embeddings'] = sentence_embeddings\n",
    "    \n",
    "    return dataset"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Getting embeddings as well for the abstract titles\n",
    "abstracts_subset = abstracts.groupby(\"labels\").head(500)\n",
    "abstracts_subset.reset_index(drop = True, inplace = True)\n",
    "\n",
    "abstract_embeddings = get_sentence_embeddings(abstracts_subset)\n",
    "save_obj(abstract_embeddings, \"abstract_embeddings\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Get PCA for the abstract titles too (turn this into a function after)\n",
    "abstract_embeddings = load_obj(\"abstract_embeddings\")\n",
    "\n",
    "#Reduce the data using PCA\n",
    "pca = PCA(.75).fit(abstract_embeddings.embeddings.to_list())\n",
    "embeddings_pca_transformed = pca.transform(abstract_embeddings.embeddings.to_list())\n",
    "\n",
    "#Define new column for new pca reduced embeddings\n",
    "abstract_embeddings['pca_embeddings'] = \"\"\n",
    "#Modify the original embeddings in the dataframe\n",
    "for index in range(len(abstract_embeddings)):\n",
    "    abstract_embeddings.pca_embeddings[index] = embeddings_pca_transformed[index]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## UMAP and Clustering"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "#Running the UMAP dimensionality reduction across all different parameters\n",
    "def umap_reduce(dataset):\n",
    "\n",
    "    #Define a range of parameters to iterate through\n",
    "    n_neighbors = [5, 10, 15, 20, 30, 50]\n",
    "    min_dist = [0.2, 0.4, 0.6, 0.8, 1]\n",
    "    n_components = [2, 3, 4]\n",
    "    metrics = ['euclidean', 'cosine']\n",
    "\n",
    "    #List of dicts, where each index corresponds to the class label\n",
    "    #Then within each one of those smaller dicts, each key will be a cluster and the value its frequency\n",
    "    dict_list = [dict() for x in range(len(dataset.labels.unique()))]\n",
    "\n",
    "    #Iterate over each parameter\n",
    "    for neighbors in tqdm(n_neighbors):\n",
    "        for dist in tqdm(min_dist):\n",
    "            for component in tqdm(n_components):\n",
    "                for metric in tqdm(metrics):\n",
    "\n",
    "                    #Set up the UMAP reducer, set a random state so reproducible\n",
    "                    reducer = umap.UMAP(random_state= 0, n_neighbors = neighbors, min_dist=dist, n_components = component, metric=metric, verbose = 0)\n",
    "\n",
    "                    #reduce the PCA-transformed embeddigns\n",
    "                    umap_embeddings = reducer.fit_transform(dataset.pca_embeddings.to_list())\n",
    "\n",
    "                    #Create a new temporary column in dataset to store UMAP embeddings\n",
    "                    dataset['umap_embeddings'] = \"\"\n",
    "                    \n",
    "                    for index in range(len(dataset)):\n",
    "                        dataset['umap_embeddings'][index] = umap_embeddings[index]\n",
    "                    \n",
    "                    #Store parameters so can send over for plotting and checking if any lead to really bad results\n",
    "                    parameters = [neighbors, dist, component, metric]\n",
    "                    #Now time to do the clustering of the UMAP projections\n",
    "                    clustered_dataset = hdb_clustering(dataset, parameters)\n",
    "\n",
    "                    #Next, analyze the clusters from this parameter run\n",
    "                    iteration_results = analyze_clusters(clustered_dataset)\n",
    "\n",
    "                    #Now need to go through iteration results.\n",
    "                    #Iterate over the list of dicts with index\n",
    "                    for key, value in iteration_results.items():\n",
    "\n",
    "                        #THIS METHOD RELIES ON THE LABEL OF THE DATA TO START AT 0\n",
    "                        #Now for each key go through the found clusters and check if they exist in overall dict (key relates to the index in the dict_list)\n",
    "                        for cluster in value:\n",
    "                            if tuple(cluster) in dict_list[key]:\n",
    "                                dict_list[key][tuple(cluster)] += 1\n",
    "                            else:\n",
    "                                dict_list[key][tuple(cluster)] = 1\n",
    "\n",
    "    #At this point should have our list of dicts ready\n",
    "    #Lets save it since this was such a time consuming process\n",
    "    save_obj(dict_list, \"dict_list\")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "#Write a function which takes in a dict_list and spits out the final CSV/Dataframe with the clusters ordered\n",
    "def order_clusters(dict_list):\n",
    "\n",
    "    #Create dataframe where to store everything\n",
    "    all_clusters = pd.DataFrame()\n",
    "\n",
    "    #Input should be a dict_list where each index of the dict refers to the label of the data\n",
    "    for idx, dictionary in enumerate(dict_list):\n",
    "        #Order the dictionary\n",
    "        ordered_dict = sorted(dictionary.items(), key = lambda x: x[1], reverse= True)\n",
    "        \n",
    "        #Filter out all entries that have less than X entries\n",
    "        filtered_dicts = []\n",
    "\n",
    "        for cluster in ordered_dict:\n",
    "            if cluster[1] >= 2 and len(cluster[0]) >=2:\n",
    "                filtered_dicts.append(cluster)\n",
    "\n",
    "        #Put into a label DF\n",
    "        label_df = pd.DataFrame(filtered_dicts, columns = [f'label_{idx}', f'freq_{idx}'])\n",
    "\n",
    "        #Concat label df into all clusters\n",
    "        all_clusters = pd.concat([all_clusters, label_df], axis=1)\n",
    "    \n",
    "    all_clusters.to_csv(\"datasets/clusters.csv\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "#Calculating the optimum min size\n",
    "def get_optimum_min_size(dataset):\n",
    "\n",
    "    #Store scores\n",
    "    scores = []\n",
    "\n",
    "    #Iterate over range of min sizes\n",
    "    for min_size in range(50):\n",
    "        if min_size > 1:\n",
    "            #set up the clusterer\n",
    "            clusterer = hdbscan.HDBSCAN(min_cluster_size = min_size)\n",
    "\n",
    "            clusterer.fit(dataset.umap_embeddings.to_list())\n",
    "\n",
    "            #Attach the probabilities to the dataset so can compute proportions\n",
    "            dataset['cluster_probabilities'] = clusterer.probabilities_\n",
    "\n",
    "            #Compute score\n",
    "            score = len(dataset.loc[dataset.cluster_probabilities < 0.05])/len(dataset)\n",
    "            score_tuple = (min_size, score)\n",
    "            scores.append(score_tuple)\n",
    "            \n",
    "\n",
    "    #Now we return the minimum size\n",
    "    return(min(scores, key = lambda t: t[1])[0])\n",
    "\n",
    "\n",
    "#Running the HDBSCAN clustering\n",
    "def hdb_clustering(dataset, parameters):\n",
    "\n",
    "    #First need to find the optimum min_size\n",
    "    optimum_size = get_optimum_min_size(dataset)\n",
    "\n",
    "    #Set up clusterer with optimum size\n",
    "    clusterer = hdbscan.HDBSCAN(min_cluster_size=optimum_size, min_samples=1)\n",
    "\n",
    "    #Form clusters\n",
    "    clusterer.fit(dataset.umap_embeddings.to_list())\n",
    "\n",
    "    #Code for plotting the clusters (uncomment to check if it works)\n",
    "    fig = plt.figure()\n",
    "    #set up colour palette\n",
    "    color_palette = sns.color_palette('deep', clusterer.labels_.max()+1)\n",
    "    cluster_colors = [color_palette[x] if x >= 0 else (0.5, 0.5, 0.5) for x in clusterer.labels_]\n",
    "    cluster_member_colors = [sns.desaturate(x, p) for x, p in zip(cluster_colors, clusterer.probabilities_)]\n",
    "    plt.scatter([e[0] for e in dataset.umap_embeddings.to_list()], [e[1] for e in dataset.umap_embeddings.to_list()], s = 50, linewidth = 0, c = cluster_member_colors, alpha = 0.25)\n",
    "    plt.title(f\"{parameters}\")\n",
    "    plt.show()\n",
    "\n",
    "    #Attach the results back to the dataset and remove some entries\n",
    "    dataset['cluster_labels'] = clusterer.labels_\n",
    "    dataset['cluster_probabilities'] = clusterer.probabilities_\n",
    "\n",
    "    #Here remove -1 cluster and probabilities less than 0.8 (???)\n",
    "    dataset = dataset.loc[dataset.cluster_labels != -1]\n",
    "    dataset = dataset.loc[dataset.cluster_probabilities > 0.8]\n",
    "\n",
    "    dataset.reset_index(drop = True, inplace = True)\n",
    "\n",
    "\n",
    "    return dataset"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "#Going through the found clusters and getting most frequent label and associated sentences\n",
    "def analyze_clusters(dataset):\n",
    "\n",
    "    #Neet to set up a dict with the keys as the labels from dataset\n",
    "    keys = list(set(dataset.labels))\n",
    "\n",
    "    labels_dict = dict([(key, []) for key in keys])\n",
    "\n",
    "    #Get list of unique clusters\n",
    "    unique_clusters = dataset.cluster_labels.unique()\n",
    "\n",
    "    #iterate over each cluster number\n",
    "    for cluster_number in unique_clusters:\n",
    "\n",
    "        #Subset based on cluster\n",
    "        cluster_subset = dataset.loc[dataset.cluster_labels == cluster_number]\n",
    "\n",
    "        #Find most common label in subset\n",
    "        max_label = mode(cluster_subset.labels)\n",
    "\n",
    "        #Subset again to just look at most common label\n",
    "        label_subset = cluster_subset.loc[cluster_subset.labels == max_label]\n",
    "\n",
    "        #Append to dict for that label\n",
    "        labels_dict[max_label].append(label_subset.text.values.tolist())\n",
    "    \n",
    "    return labels_dict\n",
    "        "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Run the UMAP reduction and clustering pipeline\n",
    "umap_reduce(abstract_embeddings)\n",
    "\n",
    "#Load and get Cluster CSV\n",
    "dict_list = load_obj(\"dict_list\")\n",
    "\n",
    "order_clusters(dict_list)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.6 64-bit"
  },
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}