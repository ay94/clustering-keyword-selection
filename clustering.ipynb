{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Inter-Class Clustering Notebook\n",
    "\n",
    "## Setting up and preparing the data for clustering\n",
    "\n",
    "Start off with loading in the two datasets which we will use for this notebook\n",
    "Using two different ones to test if the method works well across datasets"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "#Import all necessary packages\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from sentence_transformers import SentenceTransformer, models\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.decomposition import PCA\n",
    "import umap.umap_ as umap\n",
    "import seaborn as sns\n",
    "import hdbscan\n",
    "from math import *\n",
    "from statistics import *"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Preparing dataset 1: Exam Questions and Subject type\n",
    "#Loading in the questions dataset\n",
    "questions = pd.read_csv(\"datasets/questions.csv\")\n",
    "\n",
    "#Check the class numbers\n",
    "print(questions.Subject.value_counts())\n",
    "\n",
    "#Rename columns\n",
    "questions.rename(columns={'eng': 'text', 'Subject': 'labels'}, inplace = True)\n",
    "\n",
    "#Recode labels into numbers (can probably be done with sklearn method?)\n",
    "labels_mapping = {'Biology': 0, 'Chemistry' : 1, 'Maths' : 2, 'Physics': 3}\n",
    "questions.labels = questions.labels.apply(lambda x: labels_mapping[x])\n",
    "\n",
    "#Only preprocessing can be removing the new-line character (\\n)\n",
    "questions.text = questions.text.apply(lambda x: x.replace(\"\\n\", \" \"))\n",
    "\n",
    "#Lets just have a look at what the longest question is (BERT has a limit of tokens afer which it will cut off)\n",
    "lengths = questions.text.str.len()\n",
    "argmax = np.where(lengths == lengths.max())[0]\n",
    "print(questions.text.iloc[argmax].to_numpy().ravel().tolist())\n",
    "\n",
    "#Rather weird sentences and reveals that the text data contains a lot of Latex commands for formatting the text\n",
    "#TODO: come back to this and see if needs to be removed but might be a nice challenge for our method\n",
    "print(questions)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "#Preparing dataset 2: Research paper abstracts and topic types\n",
    "#Loading in the abstracts dataset\n",
    "abstracts = pd.read_csv(\"datasets/science.csv\")\n",
    "\n",
    "#Check data encoding\n",
    "# print(abstracts.head())\n",
    "#Need to change the encodings\n",
    "#A way can be to first concat the binary indicators into a list/string of numbers which then is converted to the label\n",
    "abstracts['labels'] = abstracts[abstracts.columns[3:]].apply(lambda x: \"\".join(x.astype(str)), axis = 1)\n",
    "\n",
    "#Check class balance\n",
    "print(abstracts.labels.value_counts())\n",
    "\n",
    "#Seems like there are cases where entries belong to multiple classes, remove those and just keep single members\n",
    "#Combine with mapping dict instead of making new list. A bit ugly, change later.. 0 = Physics, 1 = CS, 2 = Maths, 3 = Stats, 4 = Quantitative Biology, 5 = Quantitative Finance\n",
    "labels_mapping = {\"010000\" : 0, \"100000\" : 1, \"001000\" : 2, \"000100\" : 3, \"000010\" : 4, \"000001\" : 5}\n",
    "\n",
    "abstracts = abstracts[abstracts.labels.isin(list(labels_mapping.keys()))]\n",
    "abstracts.reset_index(drop=True, inplace= True)\n",
    "abstracts.labels = abstracts.labels.apply(lambda x: labels_mapping[x])\n",
    "\n",
    "#Rename\n",
    "abstracts.rename(columns={\"ABSTRACT\": \"text1\"}, inplace= True)\n",
    "abstracts.rename(columns={\"TITLE\": \"text\"}, inplace= True)\n",
    "\n",
    "#Only preprocessing can be removing the new-line character (\\n)\n",
    "abstracts.text = abstracts.text.apply(lambda x: x.replace(\"\\n\", \" \"))\n",
    "abstracts.text1 = abstracts.text1.apply(lambda x: x.replace(\"\\n\", \" \"))\n",
    "\n",
    "#Lets check again the longest abstract entries:\n",
    "lengths_title = abstracts.text.str.len()\n",
    "argmax = np.where(lengths_title == lengths_title.max())[0]\n",
    "\n",
    "#Check length of longest title\n",
    "print(abstracts.text.iloc[argmax].to_numpy().ravel().tolist())\n",
    "print(len(abstracts.text.iloc[argmax].to_numpy().ravel().tolist()[0].split()))\n",
    "\n",
    "lengths_abstract = abstracts.text1.str.len()\n",
    "argmax = np.where(lengths_abstract == lengths_abstract.max())[0]\n",
    "\n",
    "#Check length of longest title\n",
    "print(abstracts.text1.iloc[argmax].to_numpy().ravel().tolist())\n",
    "print(len(abstracts.text1.iloc[argmax].to_numpy().ravel().tolist()[0].split()))\n",
    "\n",
    "\n",
    "#Finally lets keep only the columns we are interested in further\n",
    "abstracts = abstracts[['text', 'text1', 'labels']]\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "010000    5120\n",
      "100000    4910\n",
      "001000    3610\n",
      "100100    2285\n",
      "000100    1636\n",
      "001100     825\n",
      "101000     682\n",
      "000010     443\n",
      "110000     437\n",
      "011000     293\n",
      "000001     209\n",
      "101100     179\n",
      "000110     105\n",
      "010100      99\n",
      "110100      36\n",
      "100010      30\n",
      "000101      24\n",
      "111000      19\n",
      "100001       9\n",
      "011100       9\n",
      "100110       5\n",
      "000011       4\n",
      "100101       2\n",
      "001101       1\n",
      "Name: labels, dtype: int64\n",
      "['Reply to Hicks et al 2017, Reply to Morrison et al 2016 Refining the relevant population in forensic voice comparison, Reply to Hicks et al 2015 The importance of distinguishing info from evidence/observations when formulating propositions']\n",
      "36\n",
      "[\"  This article is dedicated to the late Giorgio Israel. R{é}sum{é}. The aim of this article is to propose on the one hand a brief history of modeling starting from the works of Fibonacci, Robert Malthus, Pierre Francis Verhulst and then Vito Volterra and, on the other hand, to present the main hypotheses of the very famous but very little known predator-prey model elaborated in the 1920s by Volterra in order to solve a problem posed by his son-in-law, Umberto D'Ancona. It is thus shown that, contrary to a widely-held notion, Volterra's model is realistic and his seminal work laid the groundwork for modern population dynamics and mathematical ecology, including seasonality, migration, pollution and more. 1. A short history of modeling 1.1. The Malthusian model. If the rst scientic view of population growth seems to be that of Leonardo Fibonacci [2], also called Leonardo of Pisa, whose famous sequence of numbers was presented in his Liber abaci (1202) as a solution to a population growth problem, the modern foundations of population dynamics clearly date from Thomas Robert Malthus [20]. Considering an ideal population consisting of a single homogeneous animal species, that is, neglecting the variations in age, size and any periodicity for birth or mortality, and which lives alone in an invariable environment or coexists with other species without any direct or indirect inuence, he founded in 1798, with his celebrated claim Population, when unchecked, increases in a geometrical ratio, the paradigm of exponential growth. This consists in assuming that the increase of the number N (t) of individuals of this population, during a short interval of time, is proportional to N (t). This translates to the following dierential equation : (1) dN (t) dt = $\\\\epsilon$N (t) where $\\\\epsilon$ is a constant factor of proportionality that represents the growth coe-cient or growth rate. By integrating (1) we obtain the law of exponential growth or law of Malthusian growth (see Fig. 1). This law, which does not take into account the limits imposed by the environment on growth and which is in disagreement with the actual facts, had a profound inuence on Charles Darwin's work on natural selection. Indeed, Darwin [1] founded the idea of survival of the ttest on the 1. According to Frontier and Pichod-Viale [3] the correct terminology should be population kinetics, since the interaction between species cannot be represented by forces. 2. A population is dened as the set of individuals of the same species living on the same territory and able to reproduce among themselves. \"]\n",
      "419\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "At this point both datasets should be good to go for getting the embeddings. \n",
    "Now we can get the embeddings of each sentence using some BERT sentence library and ideally store the objects as pickles which we can access later (so we don't have to do this each time)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "#Helper functions for saving/loading pickle objects\n",
    "def save_obj(obj, name ):\n",
    "    with open('datasets/'+ name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name ):\n",
    "    with open('datasets/' + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "#getting sentence embeddings for all the separated questions\n",
    "def get_sentence_embeddings(dataset):\n",
    "    \n",
    "    word_embedding_model = models.Transformer('sentence-transformers/all-mpnet-base-v2', max_seq_length=384)\n",
    "    pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n",
    "\n",
    "    model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n",
    "\n",
    "    #Store the embeddings in a list\n",
    "    sentence_embeddings = []\n",
    "\n",
    "    #Go through the data and get the sentence embeddings\n",
    "    for index in tqdm(range(len(dataset))):\n",
    "        sentence = dataset.text[index]\n",
    "        embedding = model.encode(sentence)\n",
    "        sentence_embeddings.append(embedding)\n",
    "    \n",
    "\n",
    "    dataset['embeddings'] = sentence_embeddings\n",
    "    \n",
    "    return dataset"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "#lets try if it works\n",
    "\n",
    "#TOo many samples, takes forever\n",
    "#lets take a 1000 from each label\n",
    "questions_subset = questions.groupby(\"labels\").head(1000)\n",
    "questions_subset.reset_index(drop=True, inplace = True)\n",
    "\n",
    "questions_embeddings = get_sentence_embeddings(questions_subset)\n",
    "save_obj(questions_embeddings, \"questions_embeddings\")"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "196584dd8b804148803f25da8515bc79"
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4000.0), HTML(value='')))"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-5-50f9fc1610ef>:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset['embeddings'] = sentence_embeddings\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "#Getting embeddings as well for the abstract titles\n",
    "abstracts_subset = abstracts.groupby(\"labels\").head(500)\n",
    "abstracts_subset.reset_index(drop = True, inplace = True)\n",
    "\n",
    "abstract_embeddings = get_sentence_embeddings(abstracts_subset)\n",
    "save_obj(abstract_embeddings, \"abstract_embeddings\")"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f16c34e17def49f99b08beff30e5dbf8"
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2652.0), HTML(value='')))"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-6-50f9fc1610ef>:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset['embeddings'] = sentence_embeddings\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "source": [
    "#Loading in the saved dataset with emebeddings\n",
    "questions_embeddings = load_obj(\"questions_embeddings\")\n",
    "\n",
    "#Reduce the data using PCA\n",
    "pca = PCA(.50).fit(questions_embeddings.embeddings.to_list())\n",
    "embeddings_pca_transformed = pca.transform(questions_embeddings.embeddings.to_list())\n",
    "\n",
    "#Define new column for new pca reduced embeddings\n",
    "questions_embeddings['pca_embeddings'] = \"\"\n",
    "#Modify the original embeddings in the dataframe (FIX)\n",
    "for index in range(len(questions_embeddings)):\n",
    "    questions_embeddings.pca_embeddings[index] = embeddings_pca_transformed[index]\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-86-c427a3e69be8>:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  questions_embeddings.pca_embeddings[index] = embeddings_pca_transformed[index]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "#Get PCA for the abstract titles too (turn this into a function after)\n",
    "abstract_embeddings = load_obj(\"abstract_embeddings\")\n",
    "\n",
    "#Reduce the data using PCA\n",
    "pca = PCA(.75).fit(abstract_embeddings.embeddings.to_list())\n",
    "embeddings_pca_transformed = pca.transform(abstract_embeddings.embeddings.to_list())\n",
    "\n",
    "#Define new column for new pca reduced embeddings\n",
    "abstract_embeddings['pca_embeddings'] = \"\"\n",
    "#Modify the original embeddings in the dataframe\n",
    "for index in range(len(abstract_embeddings)):\n",
    "    abstract_embeddings.pca_embeddings[index] = embeddings_pca_transformed[index]\n",
    "\n",
    "print(len(abstract_embeddings.pca_embeddings[0]))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "                                                   text  \\\n",
      "0           Reconstructing Subject-Specific Effect Maps   \n",
      "1                    Rotation Invariance Neural Network   \n",
      "2     Spherical polyharmonics and Poisson kernels fo...   \n",
      "3     A finite element approximation for the stochas...   \n",
      "4     On maximizing the fundamental frequency of the...   \n",
      "...                                                 ...   \n",
      "2647  Asymmetric Connectedness of Fears in the U.S. ...   \n",
      "2648  Beta-rhythm oscillations and synchronization t...   \n",
      "2649  A metric model for the functional architecture...   \n",
      "2650  Discovering the effect of nonlocal payoff calc...   \n",
      "2651                Complex Valued Risk Diversification   \n",
      "\n",
      "                                                  text1  labels  \\\n",
      "0       Predictive models allow subject-specific inf...       1   \n",
      "1       Rotation invariance and translation invarian...       1   \n",
      "2       We introduce and develop the notion of spher...       2   \n",
      "3       The stochastic Landau--Lifshitz--Gilbert (LL...       2   \n",
      "4       Let $\\Omega \\subset \\mathbb{R}^n$ be a bound...       2   \n",
      "...                                                 ...     ...   \n",
      "2647    We study how shocks to the forward-looking e...       5   \n",
      "2648    Despite their significant functional roles, ...       4   \n",
      "2649    The purpose of this work is to construct a m...       4   \n",
      "2650    The classical idea of evolutionarily stable ...       4   \n",
      "2651    Risk diversification is one of the dominant ...       5   \n",
      "\n",
      "                                             embeddings  \n",
      "0     [-0.05915006, 0.1258094, -0.07287613, 0.022885...  \n",
      "1     [-0.058153052, 0.06509021, -0.1142524, 0.02992...  \n",
      "2     [-0.029279994, 0.11765529, 0.003319908, 0.0503...  \n",
      "3     [-0.1516673, -0.07198717, -0.07615635, 0.01972...  \n",
      "4     [-0.07824731, -0.07205073, -0.18205501, 0.1022...  \n",
      "...                                                 ...  \n",
      "2647  [-0.07261404, 0.10108422, -0.07209741, -0.0852...  \n",
      "2648  [-0.0929671, 0.021877505, -0.06163902, 0.04017...  \n",
      "2649  [-0.070025325, 0.037995797, -0.15507534, -0.00...  \n",
      "2650  [0.027584739, -0.0071246456, -0.09553093, -0.0...  \n",
      "2651  [-0.047728617, -0.017783547, -0.07363423, -0.1...  \n",
      "\n",
      "[2652 rows x 4 columns]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-10-4bfd75ffa088>:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  abstract_embeddings.pca_embeddings[index] = embeddings_pca_transformed[index]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "133\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "#Running the UMAP dimensionality reduction across all different parameters\n",
    "def umap_reduce(dataset):\n",
    "\n",
    "    #Define a range of parameters to iterate through\n",
    "    n_neighbors = [5, 10, 15, 20, 30, 50]\n",
    "    min_dist = [0.2, 0.4, 0.6, 0.8, 1]\n",
    "    n_components = [2, 3, 4]\n",
    "    metrics = ['euclidean', 'cosine']\n",
    "\n",
    "    #Need to think of a way to make a list of dict, where the index of the dict inside the list relates to the label of the data\n",
    "    #Then within each one of those smaller dicts, each key will be a cluster and the value its frequency\n",
    "    dict_list = [dict() for x in range(len(dataset.labels.unique()))]\n",
    "\n",
    "    #Iterate over each parameter\n",
    "    for neighbors in tqdm(n_neighbors):\n",
    "        for dist in tqdm(min_dist):\n",
    "            for component in tqdm(n_components):\n",
    "                for metric in tqdm(metrics):\n",
    "\n",
    "                    #Set up the UMAP reducer, set a random state so reproducible\n",
    "                    reducer = umap.UMAP(random_state= 0, n_neighbors = neighbors, min_dist=dist, n_components = component, metric=metric, verbose = 0)\n",
    "\n",
    "                    #reduce the PCA-transformed embeddigns\n",
    "                    umap_embeddings = reducer.fit_transform(dataset.pca_embeddings.to_list())\n",
    "\n",
    "                    #Create a new temporary column in dataset to store UMAP embeddings\n",
    "                    dataset['umap_embeddings'] = \"\"\n",
    "                    \n",
    "                    for index in range(len(dataset)):\n",
    "                        dataset['umap_embeddings'][index] = umap_embeddings[index]\n",
    "                    \n",
    "                    #Store parameters so can send over for plotting and checking if any lead to really bad results\n",
    "                    parameters = [neighbors, dist, component, metric]\n",
    "                    #Now time to do the clustering of the UMAP projections\n",
    "                    clustered_dataset = hdb_clustering(dataset, parameters)\n",
    "\n",
    "                    #Next, analyze the clusters from this parameter run\n",
    "                    iteration_results = analyze_clusters(clustered_dataset)\n",
    "\n",
    "                    #Now need to go through iteration results.\n",
    "                    #Iterate over the list of dicts with index\n",
    "                    for key, value in iteration_results.items():\n",
    "\n",
    "                        #THIS METHOD RELIES ON THE LABEL OF THE DATA TO START AT 0\n",
    "                        #Now for each key go through the found clusters and check if they exist in overall dict (key relates to the index in the dict_list)\n",
    "                        #Add if not\n",
    "                        for cluster in value:\n",
    "                            if tuple(cluster) in dict_list[key]:\n",
    "                                dict_list[key][tuple(cluster)] += 1\n",
    "                            else:\n",
    "                                dict_list[key][tuple(cluster)] = 1\n",
    "\n",
    "    #At this point should have our list of dicts ready\n",
    "    #Lets save it since this was such a time consuming process\n",
    "    save_obj(dict_list, \"dict_list\")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "#Write a function which takes in a dict_list and spits out the final CSV/Dataframe with the clusters ordered and merged\n",
    "def order_clusters(dict_list):\n",
    "\n",
    "    #Create dataframe where to store everything\n",
    "    all_clusters = pd.DataFrame()\n",
    "\n",
    "    #Input should be a dict_list where each index of the dict refers to the label of the data\n",
    "    for idx, dictionary in enumerate(dict_list):\n",
    "        #Order the dictionary\n",
    "        ordered_dict = sorted(dictionary.items(), key = lambda x: x[1], reverse= True)\n",
    "        \n",
    "        #Filter out all entries that have less than X entries\n",
    "        filtered_dicts = []\n",
    "\n",
    "        for cluster in ordered_dict:\n",
    "            if cluster[1] >= 2 and len(cluster[0]) >=2:\n",
    "                filtered_dicts.append(cluster)\n",
    "\n",
    "        #Put into a label DF\n",
    "        label_df = pd.DataFrame(filtered_dicts, columns = [f'label_{idx}', f'freq_{idx}'])\n",
    "\n",
    "        #Concat label df into all clusters\n",
    "        all_clusters = pd.concat([all_clusters, label_df], axis=1)\n",
    "    \n",
    "    all_clusters.to_csv(\"datasets/clusters.csv\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "#Calculating the optimum min size\n",
    "def get_optimum_min_size(dataset):\n",
    "\n",
    "    #Store scores\n",
    "    scores = []\n",
    "\n",
    "    #Iterate over range of min sizes\n",
    "    for min_size in range(50):\n",
    "        if min_size > 1:\n",
    "            #set up the clusterer\n",
    "            clusterer = hdbscan.HDBSCAN(min_cluster_size = min_size)\n",
    "\n",
    "            #TODO: Here double check if you get same results if you actually separate into the two lists, like you do for the plotting\n",
    "            clusterer.fit(dataset.umap_embeddings.to_list())\n",
    "\n",
    "            #Attach the probabilities to the dataset so can compute proportions\n",
    "            dataset['cluster_probabilities'] = clusterer.probabilities_\n",
    "\n",
    "            #Compute score\n",
    "            score = len(dataset.loc[dataset.cluster_probabilities < 0.05])/len(dataset)\n",
    "            score_tuple = (min_size, score)\n",
    "            scores.append(score_tuple)\n",
    "            \n",
    "\n",
    "    #Now we return the minimum size\n",
    "    return(min(scores, key = lambda t: t[1])[0])\n",
    "\n",
    "\n",
    "#Running the HDBSCAN clustering\n",
    "def hdb_clustering(dataset, parameters):\n",
    "\n",
    "    #First need to find the optimum min_size\n",
    "    optimum_size = get_optimum_min_size(dataset)\n",
    "\n",
    "    #Set up clusterer with optimum size\n",
    "    clusterer = hdbscan.HDBSCAN(min_cluster_size=optimum_size, min_samples=1)\n",
    "\n",
    "    #Form clusters\n",
    "    clusterer.fit(dataset.umap_embeddings.to_list())\n",
    "\n",
    "    #Code for plotting the clusters (uncomment to check if it works)\n",
    "    fig = plt.figure()\n",
    "    #set up colour palette\n",
    "    color_palette = sns.color_palette('deep', clusterer.labels_.max()+1)\n",
    "    cluster_colors = [color_palette[x] if x >= 0 else (0.5, 0.5, 0.5) for x in clusterer.labels_]\n",
    "    cluster_member_colors = [sns.desaturate(x, p) for x, p in zip(cluster_colors, clusterer.probabilities_)]\n",
    "    plt.scatter([e[0] for e in dataset.umap_embeddings.to_list()], [e[1] for e in dataset.umap_embeddings.to_list()], s = 50, linewidth = 0, c = cluster_member_colors, alpha = 0.25)\n",
    "    plt.title(f\"{parameters}\")\n",
    "    plt.show()\n",
    "\n",
    "    #Attach the results back to the dataset and remove some entries\n",
    "    dataset['cluster_labels'] = clusterer.labels_\n",
    "    dataset['cluster_probabilities'] = clusterer.probabilities_\n",
    "\n",
    "    #Here remove -1 cluster and probabilities less than 0.8 (???)\n",
    "    dataset = dataset.loc[dataset.cluster_labels != -1]\n",
    "    dataset = dataset.loc[dataset.cluster_probabilities > 0.8]\n",
    "\n",
    "    dataset.reset_index(drop = True, inplace = True)\n",
    "\n",
    "\n",
    "    return dataset"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "#Going through the found clusters and getting most frequent label and associated sentences\n",
    "def analyze_clusters(dataset):\n",
    "\n",
    "    #Neet to set up a dict with the keys as the labels from dataset\n",
    "    keys = list(set(dataset.labels))\n",
    "\n",
    "    labels_dict = dict([(key, []) for key in keys])\n",
    "\n",
    "    #Get list of unique clusters\n",
    "    unique_clusters = dataset.cluster_labels.unique()\n",
    "\n",
    "    #iterate over each cluster number\n",
    "    for cluster_number in unique_clusters:\n",
    "\n",
    "        #Subset based on cluster\n",
    "        cluster_subset = dataset.loc[dataset.cluster_labels == cluster_number]\n",
    "\n",
    "        #Find most common label in subset\n",
    "        max_label = mode(cluster_subset.labels)\n",
    "\n",
    "        #Subset again to just look at most common label\n",
    "        label_subset = cluster_subset.loc[cluster_subset.labels == max_label]\n",
    "\n",
    "        #Append to dict for that label\n",
    "        labels_dict[max_label].append(label_subset.text.values.tolist())\n",
    "    \n",
    "    return labels_dict\n",
    "        "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Run the UMAP reduction and clustering pipeline\n",
    "umap_reduce(abstract_embeddings)\n",
    "\n",
    "#Load and get some clusters\n",
    "dict_list = load_obj(\"dict_list\")\n",
    "\n",
    "order_clusters(dict_list)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Seems like a pretty non-trivial problem actually...\n",
    "#Try and merge the clusters\n",
    "def merge_clusters(dict_list):\n",
    "\n",
    "    \n",
    "    for idx, dictionary in enumerate(dict_list):\n",
    "        \n",
    "        #FOr each dictionary we want a list of clusters on which we can merge afterwards\n",
    "        list_of_clusters = []\n",
    "\n",
    "        #Order the dictionary (and to get it in a tuple format)\n",
    "        ordered_dict = sorted(dictionary.items(), key = lambda x: x[1], reverse= True)\n",
    "        \n",
    "        #Go over each cluster\n",
    "        for cluster in ordered_dict:\n",
    "            #Just looking at ones which have some value to us\n",
    "            if cluster[1] >= 2 and len(cluster[0]) >=2:\n",
    "\n",
    "                #Add them to the list of clusters for this dict\n",
    "                list_of_clusters.append(list(cluster[0]))\n",
    "\n",
    "\n",
    "\n",
    "dict_list = load_obj(\"dict_list\")\n",
    "\n",
    "merge_clusters(dict_list)\n"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.6 64-bit"
  },
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}